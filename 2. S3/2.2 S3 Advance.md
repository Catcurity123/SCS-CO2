#### A. S3 Encryption
(+) Bucket are not encrypted only objects are encrypted
(+) There are `client-side encryption` and `server-side encryption`. Both of them are encryption at rest.

(+) `Client-side encryption` refers to the practice of the client encrypted the data before uploading onto storage medium (S3 in this case), after uploading onto S3, the data will be encrypted again by AWS and stored on physical S3 storage. During this whole process, AWS can not make sense of any of the uploaded data as it is encrypted the whole time.
(+) `Server-side encryption` refers to the practice of client upload the unencrypted data onto storage medium (S3 in this case), after uploading onto S3, the data will be encrypted by AWS and stored onphysical S3 storage. Although the data will be encrypted and protected during transmission, it is still visable at S3 endpoint.

![[Pasted image 20240104125232.png | 600]]

(+) There are 3 types of `server-side encryption`: `Server-side Encryption with Customer-Provided Keys (SSE-C)`, `Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)`, and `Server-Side Encrytion with Customer Master Keys (CKMs) Stored in AWS Key Management Service (SSE-KMS)`

(+) With `SSE-C`: The customer is responsible for `encryption keys` and AWS responsible for the `encryption and decryption process`. In this case, we are offloading the encryption process (which take computing power) to S3, but we still need to handle the creation and management of `encrypted keys.`
![[Pasted image 20240104125907.png | 600]]

(+) With `SSE-S3 (AES256)`: With this method, S3 manages both the creation and management of keys as well as the encryption and decryption process of the data.
The user only needs to provide S3 with the data. With this method, S3 creates a master key and create for the encryption and decryption process, the user has no influence over this and it is done behind the scene.
(+) This is the standard method of encryption on S3, but it has 3 major drawbacks:
	(-) This is not suitable for regulatory environment where we need to `control access to keys`
	(-) This is not suitable when we need to control the `rotation of keys` according to business logic.
	(-) This is not suitable when we need to define `role-separtion`, as the S3 admin can view the key, and use the key to encrypt or decrypt the data. This is not suitatble as business logic define the `administrative grou`p (which control the system) and `functional group` (which use the system), most of the case the `administrative group` can not perform actions provided for `functional group`.

![[Pasted image 20240104130640.png | 600]]

(+) With `SSE-KMS`: In this case, AWS also handles the creation, management and encryption-decryption process of the data. However, while `SSE-S3` handle encrytion process underline, in this situation we will create a CMK on KMS which will be used to create DEK for every object uploaded on S3. Meaning that whenever we upload data onto S3, S3 will be provided with a DEK created by CMK (created by us) to encrypt the data. Therefore, we can control the management of the CMK (the permission of it using key policies), the key rotation and subsequently achieve role-separation because when we want to decrypt the data using `SSE-KMS` we need to have access to the created CMK.

![[Pasted image 20240104131514.png | 600]]
(+) `Bucket Default Encryption`: If set, objects will use the default encryption if we dont explicitly specifies the encryption we wanted.

(+) `Summary`
![[Pasted image 20240104131811.png | 600]]


#### B. S3 Performance Optimization
`Singlle PUT Upload`
(+) The default way of uploading data onto S3 is a `single data stream` from user to s3, meaning that if the stream fails the upload fails and a full restart is required.
(+) The speed and & reliability is limited of 1 streams. This is opposite to downloading data online where it utilizes multiple streams to download and upload data.
(+) Any upload is limited to 5GB.

==> Therefore, distributed transfer of data is being developed to address this issue

![[Pasted image 20240103194827.png]]

(+) There is a solution for this called `Multipart upload`

`Multipart Upload`
(+) Data is broken up, with the min data size of 100MB for multipart. When uploading the maximum parts being broken up is 10,000 max parts, ranging from 5MB -> 5GB.
(+) The last part of `Multipart Upload` can be smaller than 5MB. So parts can fail individually and can be restarted individually.
(+) This means that the transfer rate is equal to the speeds of all parts as they are uploaded individually.

![[Pasted image 20240103195939.png]]

`S3 Byte-Range Fetches`
(+) Parallelize `downloads` by specifying byte ranges.
(+) If there is a failure in the download, its only for a specific byte range.
![[Pasted image 20240222135633 1 1 1.png]]
(+) Used to speed up downloads, and used to download partial amounts of the file (header information)

`S3 Accelerated Transfer (OFF)`
![[Pasted image 20240103200114.png]]

`S3 Accelerated Transfer (ON)`
(+) This feature is switched off by default, the limitation of using this feature is that `the bucket name can not contain period` and `bucket name has to be bucketname-compatible`.
(+) The way this works is that the uploaded data is transfered to the closest `Edge Location` not directly to S3. After that `Edge Location` transfer the data over `AWS Global Network` which is maintained by AWS and tend to be a direct link between the `Edge Location` to other AWS's network.

![[Pasted image 20240103200442.png]]

`NOTE`
(+) It is important to understand that the Internet is a `global, interconnect` network with many stops along the way to enable consistency and redundancy. The Internet is much like a public transport with multiple stops for general public usage.
(+) The `AWS Network` though is purposed-built to link one region to another region in the AWS network. So it is much like an express train, meaning that it is faster and with lower consistent latency.

#### C. S3 Replication
(+) Replication of S3 allows for `Cross-Region Replication (CRR)` and `Same-Region Replication (SRR)`
![[Pasted image 20240104145611.png | 600]]

(+) Replication configuration is enabled on Source bucket,  a role is needed for S3 to assume so as to perform replication on the destination bucket.
(+) There is one major difference between replication over different account. 
	(-) For replication in the same account, both the source and the destination bucket is owned by the same account, so they both trust account that they are in, they both trust the IAM, and both trust the role. So the role can access both buckets.
	(-) For replication in different account, the destination account is in a different account, so it does not trust the account of the source bucket, meaing that it does not trust the IAM, so it does not trust the role. Meaning the role can not be used to create replication on the destination bucket. To fix this, we need to add a permission in the bucket policy so as to allows the role in the source account to create replication on the destination bucket.

###### S3 Replication Options
(+) We can create replication for all objects or a subset of objects, we can also select the `storage class` for the object.
(+) About the `ownership` of the objects being replicated, the default is that the replicated object is own by the source bucket, meaing that the destination bucket can not view or delete the objects. We can override this so that the ownership is changed to the destination bucket.
(+) `Replication Time Control (RTC)` is to ensure the data between the source and destination is always be in sync.

`NOTE`
(+) Replication is `not retroactive` meaning that the existing data will not be replicated only the newy created.
(+) Replication requires `Versioning to be enabled`.
(+) Replication is `One-way` Source to Destination
(+) Replication support `Unencrypted, SSE-S3, and SSE-KMS`, not `SSE-C` as the keys is not in charge of AWS.
(+) Source bucket needs permission to objects.
(+) Replication do not include system events, objects in Glacier or Glacier Deep Archive.

`Why we use replication`
(+) SRR can be used for log aggregation from different buckets, syncing data from PROD and TEST environment.
(+) SRR might be needed for resilience with strict sovereignty where data must be gathered in one bucket.
(+) CRR can be used for global resilience improvement, or to improve latency reduction for users in different regions.

#### D. S3 Lifecycle Configuration
(+) A lifecycle configuration is a `set of rules consist of actions` on a `Bucket` or `groups of objects` that need `transition actions` into `different states` or `expiration actions`. 
(+) Lifecycle management automates moving your objects between the different storage tiers, thereby maximizing cost effectiveness.
(+) Combining lifecycle mangement with versioning to maximize effectiveness
###### 1. Transition flow
![[Pasted image 20240104143107.png | 700]]

#### E. S3 Object Lock
(+) S3 Object Lock can be used to store objects using a `write once, read many (WORM)` model. It can help prevent objects from being deleted or modified for a fixed amount of time or indefinitely
(+) S3 Object Lock can be used t omeet regulatory requirements that require`WORM` storage, or add an extra layer of protection against object changes and deletion.
![[Pasted image 20240221170716.png]]
![[Pasted image 20240221170736.png]]

###### Retention period
![[Pasted image 20240222132552.png]]

![[Pasted image 20240222132726.png]]

#### F. Glacier Vault Lock
(+) S3 Glacher Vault Lock allows you to easily deploy and enforce comliance controls for individual S3 Glacier vaults with a vault lock policy.

###### Summary
(+) Use `S3 Object Lock` to store objects using a write once, read many (WORM) model
(+) Object Lock can be on `individual objects` or applied `across the bucket` as a whole.
(+) Object Lock comes in two modes: `governance mode` and `compliance mode`.
![[Pasted image 20240222133104.png]]

(+) S3 Glacier Vault Lock allows you to `easily deploy` and `enforce complicante controls for individua S3 Glacier vaults with a vault lock policy
(+) You can `specify controls`, such as `WORM`, in a `vault lock policy` and `lock the policy from future edits`. Once locked, the policy can no longer be changed.